{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      "sepal.length    150 non-null float64\n",
      "sepal.width     150 non-null float64\n",
      "petal.length    150 non-null float64\n",
      "petal.width     150 non-null float64\n",
      "variety         150 non-null int64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 6.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "link = 'https://raw.githubusercontent.com/maksymsur/spltr/master/dataset/iris_num.csv'\n",
    "db = pd.read_csv(link)\n",
    "print(db.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By building a neural network we'll try to predict a 'variety' column. This column identifies Iris species: Setosa = 0, Versicolor = 1 and Virginica = 2. Let's verify that it is comprised from the mentioned 3 unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(db.variety.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:** Making **`X`** (Input) dataset and excluding the 5th 'variety' column (our Target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal.length  sepal.width  petal.length  petal.width\n",
      "0           5.1          3.5           1.4          0.2\n",
      "1           4.9          3.0           1.4          0.2\n",
      "2           4.7          3.2           1.3          0.2\n",
      "3           4.6          3.1           1.5          0.2\n",
      "4           5.0          3.6           1.4          0.2\n"
     ]
    }
   ],
   "source": [
    "base_db = db.iloc[:,:-1]\n",
    "print(base_db.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:** Building **`y`** (Target) dataset out of 'variety' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: variety, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_db = db.iloc[:,-1]\n",
    "print(target_db.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4:** Loading necessary packages including `Spltr`. Note: for `Spltr` to work properly `torch`, `numpy` and `pandas` shall be installed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from spltr import Spltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5.1:** Now let's instantiate a Spltr object by including **`X,y`** datasets into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splt = Spltr(base_db,target_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5.2:** Alternatively we may load whole datasets and apply basic preprocessing scenarios (exclude columns, change shape and type of the data, etc.). That allows to quickly iterate and find the best approach to current ML task. To demonstrate that, let's reinstantiate `base_db` & `target_db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_db = pd.read_csv(link) # Now 'base_db' is a 5-column dataset that includes the 'target' (5th) column.\n",
    "target_db = link # And 'target_db' is a simple link to the same 5-column dataset.\n",
    "\n",
    "splt = Spltr(base_db,target_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start with processing **`X`** by selecting only 4 feature columns and, thus, excluding the target (5th column). Pls note that presented vocabulary-type selection `{column start : column finish}` works only for DataFrames as for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X]: The following DataFrame was loaded:\n",
      "\n",
      "   sepal.length  sepal.width  petal.length  petal.width\n",
      "0           5.1          3.5           1.4          0.2\n",
      "1           4.9          3.0           1.4          0.2\n",
      "\n",
      "[X]: Tensor of shape torch.Size([150, 4]) was created\n"
     ]
    }
   ],
   "source": [
    "splt.process_x(preview=True, tensor_dtype=torch.float64, usecols={0:4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continue with processing **`y`** by using just the 5th column named 'variety'. Note that CSV columns may be selected as per official pd.read_csv documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[y]: The following CSV was loaded:\n",
      "\n",
      "   variety\n",
      "0        0\n",
      "1        0\n",
      "\n",
      "[y]: Tensor of shape torch.Size([150, 1]) was created\n"
     ]
    }
   ],
   "source": [
    "splt.process_y(preview=True, usecols=['variety'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "splt.reshape_xy(y_shape=-1) # Reshaping 'y' to be easily used in a classification task\n",
    "print(splt.shape_y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6:** Splitting the dataset into train - 30%, test - 20%, which makes validation set equal to 50% (calculated automatically: val = 100% - train - test), and initializing permutation of the data. \n",
    "\n",
    "`Spltr.split_data` method may use native `DataLoader` methods like `num_workers`, `pin_memory`, `worker_init_fn`, `multiprocessing_context` and others as per PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X,y]: The Data is splitted into 3 datasets of length: Train 45, Test 30, Validation 75.\n"
     ]
    }
   ],
   "source": [
    "splt.split_data(splits=(0.3,0.2), perm_seed=3, batch_size=1, num_workers = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 7:** Now let's clean unnecessary variables saved in the memory. This step may be especially useful if you are dealing with a huge datasets and don't want for `X,y tensors` to share the memory with `X,y DataLoader objects`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables are deleted. Only Train-Test (Validation) sets are left.\n"
     ]
    }
   ],
   "source": [
    "splt.clean_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 8:** Setting up a very simple neural network. Pls mind that the presented network architecture is comprised only to demonstrate how `Spltr` may be adopted. That's not an optimal way to solve Iris classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train loss:  1.09528\n",
      "Testing: Correctly identified samples 21 out of 30\n",
      "\n",
      "Epoch 2. Train loss:  0.80778\n",
      "Testing: Correctly identified samples 21 out of 30\n",
      "\n",
      "Epoch 3. Train loss:  0.58353\n",
      "Testing: Correctly identified samples 24 out of 30\n",
      "\n",
      "Epoch 4. Train loss:  0.48647\n",
      "Testing: Correctly identified samples 26 out of 30\n",
      "\n",
      "Epoch 5. Train loss:  0.43741\n",
      "Testing: Correctly identified samples 21 out of 30\n",
      "\n",
      "Epoch 6. Train loss:  0.41516\n",
      "Testing: Correctly identified samples 27 out of 30\n",
      "\n",
      "Epoch 7. Train loss:  0.38361\n",
      "Testing: Correctly identified samples 30 out of 30\n",
      "\n",
      "VALIDATION: Correctly identified samples 73 out of 75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.input = nn.Linear(4,8)\n",
    "        self.output = nn.Linear(8,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = F.softsign(self.input(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.9, eps=1e-08)\n",
    "\n",
    "for n in range(7):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Fitting a dataset for Training\n",
    "    \n",
    "    for train_data, train_target in splt.xy_train:\n",
    "        model.zero_grad()\n",
    "        train_result = model(train_data.float())\n",
    "        loss = criterion(train_result, train_target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    test_correct = 0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Fitting a dataset for Testing\n",
    "        \n",
    "        for test_data, test_target in splt.xy_test:\n",
    "            test_result = model(test_data.float())\n",
    "            _, predicted = torch.max(test_result, 1)\n",
    "            test_correct += (predicted == test_target).sum().item()\n",
    "            \n",
    "    print(f'Epoch {n+1}. Train loss: ', round(train_loss/len(splt.xy_train), 5))\n",
    "    print(f'Testing: Correctly identified samples {test_correct} out of {len(splt.xy_test)}', end='\\n\\n')\n",
    "    \n",
    "val_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Fitting a dataset for Validation\n",
    "    \n",
    "    for val_data, val_target in splt.xy_val:\n",
    "        val_result = model(val_data.float())\n",
    "        _, val_predicted = torch.max(val_result, 1)\n",
    "        val_correct += (val_predicted == val_target).sum().item()\n",
    "            \n",
    "print(f'VALIDATION: Correctly identified samples {val_correct} out of {len(splt.xy_val)}', end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
